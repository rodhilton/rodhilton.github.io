---
layout: post
title: 'Backwards Compatibility: The Language Killer (Part 1)'
categories:
- Programming
tags: []
status: publish
type: post
published: true
meta:
  dsq_thread_id: '607678625'
author:
  login: air0day
  email: blog@air0day.com
  display_name: Rod Hilton
  first_name: Rod
  last_name: Hilton
---
<p>Every professional software developer knows that, no matter how long and how carefully you design your application, even if you make it so modular and object-oriented that even the most complex change is trivial, eventually something will come along that won't work with your design.  You are then forced to either duct tape a solution together or redesign the affected components, hopefully as little as possible.</p>
<p>It's clear from looking at the evolution of nearly any programming language that this problem occurs within programming languages as well.  Languages, however, tend to suffer from an affliction that most production and end-user software systems do not: an obsession with maintaining backwards compatibility.</p>
<p><!--more--></p>
<p>It's extremely reasonable to want things to be backwards-compatible.  Your e-mail client should be able to read your old e-mail even when you upgrade, for example.  We, as users, demand backwards compatibility in our software.  When companies fail to provide it, they typically encounter a great deal of outrage from consumers.  This is hard to ignore, so backwards compatibility is extremely important.</p>
<p>Let's take a hypothetical example, however, of when it becomes more trouble than it's worth.  Let's say you work on the design of a console gaming system.  Your system uses CDs for storage, so your system has a cd reader in it.  Easy enough.  Then you work on the next generation system and it uses DVDs.  Fine, your DVD drive can read CDs as well, so you put an emulator into the system that allows people to run their old games, making them happy.  Then you work on the next system, but you encounter a problem.  Disc read speeds have capped out, and the levels of most games are too large; people won't be willing to wait for a disc reader to read them.  So your team decides to move to flash-based storage, like game cartridges.  Now you're in trouble.  "No, no, it's okay," you tell your boss.  "We'll include a special cartridge that can hold a CD or DVD, and it will have a small disc reader inside of it, so the console will think its a regular cartridge, and people will still be able to play their old games."  As you can see, you're sort of duct-taping a solution into place now.  But it works, the boss is happy, and customers are happy.  But oh shit, the latest version of the console system uses extra-tiny flash cartridges, because the smaller discs are more resistant to heat, and overheating was a problem for the last version.  Now the carts are too small for your cd idea.  What do you do?  </p>
<p>Unsurprisingly, the solution in the industry seems to be one of two options: drop the backwards-compatibility of the system, or give up on the improvement for the sake of backwards-compatibility.  Perhaps figure out a different, less-efficient way of dealing with the heat problem and keep the cartridges large enough for a special cart to hold a disc.  </p>
<p>This second option tends to be the road taken by language designers.  Rather than simply say that previous users of a language change their source code (which would almost certainly result in a lower adoption rate of new versions), language designers will maintain the backwards-compatibility of the system.  When this occurs over time, the language design itself tends to deteriorate.  You can also see it occur in third party libraries.  Slow deprecation is helpful, but only for something like an API, it tends to never work for the language itself.</p>
<p>Take C, for example.  C is an excellent low-level programming language for systems.  Code is very fast, and the programmer is given access to very deep system-level functions, such as the ability to send operating system signals.  C exists to create code that's as close to Assembly as possible without having to deal with registers, and it accomplishes this task beautifully.  Eventually, however, the object oriented paradigm began to grow in popularity.  Object-oriented programming allowed for a high level of modularity in code, which was great for program maintenance.  OOP encouraged high cohesion and low coupling between modules.  In ideal OOP development, code tends to almost write itself once you develop well-designed data structures.  So OOP was hacked onto C to become C++.  But rather than making a C-like language built upon objects, the designers were concerned with backwards compatibility, so a C++ compiler had to recognize regular C code.  This meant that things like global variables had to stay around, and include files would still be handled by the preprocessor rather than behaving more like modules that could be imported.  The end result is a language that seems somewhat clumsy in design, allowing both object oriented code while still compiling code that completely violates OOP paradigms.  Later still, things like namespaces and templates were added to attempt to address the problem, but those also behaved largely as a hack.</p>
<p>When Java was designed, an attempt was made to remove the problems of C/C++.  Object Oriented principles are enforced at the compiler level, and there is no functionality for global variables whatsoever.  Other files are not included verbatim, but rather elements from packages can be imported into a namespace.  Templates were no longer essential, because all objects would be subclasses of java.lang.Object, so any sort of Data Structure you wrote could deal with Objects and be usable for anything.  But it was not perfect.  Rather than having EVERY data type inherit from Object, primitive types were left alone, staying similar to their C predecessors.  Thus, a List of objects could not contain ints.  This was dealt with by providing wrapper classes, but even that was starting to look like a hack.</p>
<p>Languages like Ruby have come along, which treat EVERYTHING like an object.  Python considers other modules to, themselves, be objects.  But notice how each of these improvements dictates a complete branch from the original language, creating a new language?  </p>
<p>Java 5 didn't come out with primitive types removed and replaced with objects.  It was released with autoboxing and unboxing, which performs a compile time check that allows you to use an int as an Object and have it automatically wrapped (boxed) inside an Integer object.  C++ didn't do away with global variables, it allowed them as though it didn't break what C++ was setting out to accomplish.</p>
<p>And, with time, the languages begin to buckle under their design.  As the industry evolves, a language attempts to evolve with it, but it must always maintain backwards compatibility, so it deteriorates as it adds features, rather than simply improves.  It's not surprising: when Java 1.4 added the "assert" function, thereby making "assert" a reserved word, the compiler shipped with the option OFF by default, so as not to anger developers using variables named "assert".  They did so with good reason: the addition of the enum type in Java 5 has met with considerable criticism, and many companies have become slow-adopters because they don't want to screw around with renaming variables or packages named "enum" (for example, companies using Axis for web service deployment had to wait a while after the release of Java 5 for Axis to deprecate and replace a package named enum, because old Axis code wouldn't compile in Java 5).</p>
<p>But, as we've seen time and time again, when a language has deteriorated enough, a new language is formed with many of the same principles, but solving the problems introduced by the age of the previous language.  This is far worse than a new version of the language which isn't backwards-compatible: it's an entirely new language that must be learned and utilized.  People are hailing Ruby as the "Next Java" just as they hailed Java as the "Next C++", but the principles have hardly changed - it's really just building a new road to the same destination because the road crew was unwilling to patch its own potholes for fear that people would stop driving on it.  But now people are slowly learning to take this new road and avoid the old one entirely, since the new one is new enough to be free from potholes.  Eventually, however, it will deteriorate and someone will, once again, build a new road.</p>
<p>It's maddening.</p>
